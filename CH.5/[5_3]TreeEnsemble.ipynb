{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **앙상블(Ensemble) 학습**\n",
        "\n",
        "- 정형 데이터를 다루는 데 가장 뛰어난 알고리즘\n",
        "\n",
        "<br>\n",
        "\n",
        "## **1. 랜덤 포레스트(Random Forest)**\n",
        "\n",
        "- 결정 트리를 랜덤하게 만들어 결정 트리의 숲을 만든다\n",
        "- 각 결정 트리의 예측을 사용해 최종 예측을 만든다\n",
        "- 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 골라 최선의 분할을 찾는다\n",
        "- RandomForestClassifier 클래스\n",
        "  - 먼저 각 트리를 훈련하기 위한 데이터를 랜덤하게 만든다\n",
        "  - **부트스트랩 샘플(bootstrap sample)** : 입력한 훈련 데이터에서 샘플을 추출하여 훈련 데이터를 만드는 것. 중복되어 추출될 수 있음\n",
        "  - 예. 1,000개의 샘플이 있는 가방에서 100개를 뽑는다면, 먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는다\n",
        "  - 부트스트랩 샘플은 훈련 세트와 크기가 같게 만든다\n"
      ],
      "metadata": {
        "id": "aBOSOdYSENKV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dP7F3wYYEHwz"
      },
      "outputs": [],
      "source": [
        "# 화이트 와인 분류 데이터에 랜덤 포레스트 적용\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# return_train_score=True는 검증 점수와 훈련 세트 점수도 같이 반환(기본값은 False)\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNULYZOuLWs-",
        "outputId": "be7a3d95-d91e-4472-a219-06fc21b7ff4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤 포레스트는 결정 트리의 앙상블이기 때문에 DecisionTreeClassifier의 중요 매개변수가 모두 존재함\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgZQQhM6MQ46",
        "outputId": "0aee6b2c-ae16-4f2a-f945-fb9903914710"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 OOB 샘플(Out of Bag sample)**\n",
        "\n",
        "- 랜덤 포레스트 사용 시 부트스트랩 샘플에 포함되지 않은 남은 샘플\n",
        "- 이 샘플로 훈련된 모델을 평가할 수 있음 = 검증 세트\n",
        "- 이 점수를 얻으려면 oob_score=True 지정\n"
      ],
      "metadata": {
        "id": "4et9l17hMxW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "rf.fit(train_input, train_target)\n",
        "# oob_score를 출력하면 교차 검증을 대신할 수 있다\n",
        "print(rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3QDoQTcMk6I",
        "outputId": "20bb64f4-05a5-45da-b10e-7ebe329621ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. 엑스트라 트리(Extra Tree)**\n",
        "\n",
        "- 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용\n",
        "  - 기본적으로 100개의 트리 훈련\n",
        "- 랜덤 포레스트와 비슷하지만 부트스트랩 샘플을 사용하지 않음\n",
        "- 노드를 분할할 때 무작위로 분할함\n",
        "  - 그러면 성능이 떨어지지 않나? -> 많은 수의 트리를 앙상블하기 때문에 오히려 과적합을 방지하고 검증 세트의 점수를 높임\n",
        "  - 무작위성 덕분에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야하지만 계산은 더 빠름\n",
        "- DecisionTreeClassifier에서 splitter 인자를 random으로 주었다면 이것이 엑스트라 트리\n",
        "- ExtraTreesClassifier 클래스\n",
        "  - 회귀 버전은 ExtraTreesRegressor\n"
      ],
      "metadata": {
        "id": "jGuIx_9QNfVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "id": "iBiPcV0HNW75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81634b56-a897-4ac9-8ff5-46cb526f49cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 중요도 확인\n",
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0jCirU_y0Vn",
        "outputId": "cd68c8cf-b524-4074-83bd-e7d4e4907198"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. 그레이디언트 부스팅(Gradient Boosting)**\n",
        "\n",
        "- 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식\n",
        "- 기본적으로 깊이 3, 결정 트리 100개 이용\n",
        "- 과대적합에 강하고 일반화 성능이 높음\n",
        "- 경사 하강법을 사용하여 트리를 앙상블에 추가\n",
        "  - 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동 -> 그래서 깊이가 얕은 트리를 사용\n",
        "  - 성능은 뛰어나지만 속도는 앞선 두 알고리즘보다 느림\n",
        "- 분류에서는 로지스틱 손실 함수, 회귀에서는 평균 제곱 오차 함수 사용\n",
        "- GradientBoostingClassifier 클래스, 회귀 버전은 GradientBoostingRegressor\n",
        "  - subsample은 사용할 훈련 세트의 비율을 정함 -> 1보다 작으면 훈련 세트의 일부만 사용\n",
        "  - 학습률 매개변수로 속도 조절\n",
        "  - n_jobs는 없음 -> 순서대로 트리를 추가하기 때문에 속도가 느림\n"
      ],
      "metadata": {
        "id": "LW2xJN4jzsLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "# 과대적합에 강한 모습\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raXLRT4Azcvs",
        "outputId": "ee9fc98f-057d-476b-b0b3-e3f37dc4667f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습률(기본값=0.1)을 높이고 트리 개수를 증가시키면 성능이 향상될 수 있음\n",
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4IydiG44_zm",
        "outputId": "030e52e6-5768-44f4-c64a-14ce18164417"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 중요도 확인\n",
        "# 랜덤 포레스트나 엑스트라 트리와는 달리 일부 특성에 더 의존함\n",
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEftR2j85avJ",
        "outputId": "ce834afd-a764-462e-94d1-1eb64a77e1f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. 히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)**\n",
        "\n",
        "- 그레이디언트 부스팅의 속도와 성능을 개선한 버전\n",
        "- 먼저 입력 특성을 256개의 구간으로 나눔\n",
        "- 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용함 -> 누락된 특성이 있더라도 전처리 필요x\n",
        "- HistGradientBoostingClassifier 클래스, 회귀 버전은 HistGradientBoostingRegressor\n",
        "- 성능 향상을 위한 매개변수\n",
        "  - max_iter : 부스팅 반복 횟수 지정(트리의 개수를 지정)"
      ],
      "metadata": {
        "id": "DoFKe_FI6kv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n",
        "# 과대적합을 억압하면서 그레이디언트 부스팅보다는 조금 나은 성능을 보임\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGoWeFyc5p68",
        "outputId": "eaa45ad6-b544-4f44-91a3-b1e0c79a0e03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 중요도 확인을 위해 permutation_importance() 함수 사용\n",
        "# permutation_importance() : 특성을 하나씩 랜덤하게 섞어 모델의 성능이 변화하는지 관찰하여 어떤 특성이 중요한지 계산\n",
        "# 테스트 세트에도 적용 가능\n",
        "# n_repeats 매개변수 : 랜덤하게 섞을 횟수를 지정\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "# permutation_importance() 반환 객체 : 특성 중요도(importances), 평균(importances_mean), 표준편차(importances_std)\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-xhegZV72EO",
        "outputId": "0d7fff02-7225-46d8-b09b-2205fe2b62be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 세트의 특성 중요도 확인\n",
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn4wMS4d9eH1",
        "outputId": "487f4240-98a9-4d0a-e3f5-c5f963928840"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 성능 확인\n",
        "hgb.score(test_input, test_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fpv-5g79E1q",
        "outputId": "78a19947-ab14-4fce-d7dc-b9fdabc5d49a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 XGBoost**\n",
        "\n",
        "- 사이킷런이 아닌 그레이디언트 부스팅 알고리즘을 구현한 라이브러리\n",
        "- cross_validate함수와 함께 사용 가능\n",
        "- tree_method 매개변수를 hist로 지정하면 히스토그램 기반 그레이디언트 부스팅을 사용할 수 있음\n",
        "\n",
        "### **4.2 LightGBM**\n",
        "\n",
        "- 마이크로소프트에서 만든 히스토그램 기반 그레이디언트 부스팅 라이브러리\n",
        "- 사이킷런의 라이브러리가 영향을 많이 받음"
      ],
      "metadata": {
        "id": "0sWXDkGM93pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE5QrdWV9ZX9",
        "outputId": "c118c9ff-41a2-416e-ff94-3fa4ddc8a974"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9558403027491312 0.8782000074035686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLsllRN-g-L",
        "outputId": "aa973f45-8685-49bc-f1ac-cded7ca6d754"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.935828414851749 0.8801251203079884\n"
          ]
        }
      ]
    }
  ]
}